{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a \"fake news\" classifier\n",
    "\n",
    "I this project, we will apply the basics of NLP (Natural Language Processing) along with some supervised machine learning to build a \"fake news\" detector. We'll begin by doing the basics of supervised machine learning, and then move forward by choosing a few important features and testing ideas to identify and classify fake news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll use pandas alongside scikit-learn to create a sparse text vectorizer to train and test a supervised model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0                                              title  \\\n",
      "0       8476                       You Can Smell Hillary’s Fear   \n",
      "1      10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2       3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3      10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4        875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n",
      "['00', '000', '0000', '00000031', '000035', '00006', '0002', '000billion', '000ft', '001']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "#read csv file\n",
    "df = pd.read_csv('fake_or_real.csv')\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=89)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the sparse CountVectorizer created in the previous exercise, we'll work on creating tf-idf vectors for the documents. we'll set up a TfidfVectorizer and investigate some of its features.\n",
    "\n",
    "In this exercise, we'll use pandas and sklearn along with the same X_train, y_train and X_test, y_test DataFrames and Series we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0002', '000billion', '000ft', '001']\n",
      "[[0.         0.01855224 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of how the vectors work, we'll investigate them by converting them into pandas DataFrames.\n",
    "\n",
    "Here, we'll use the same data structures we created in the previous (count_train, count_vectorizer, tfidf_train, tfidf_vectorizer) as well as pandas, which is imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000  00000031  000035  00006  0002  000billion  000ft  001  ...  \\\n",
      "0   0    1     0         0       0      0     0           0      0    0  ...   \n",
      "1   0    0     0         0       0      0     0           0      0    0  ...   \n",
      "2   0    0     0         0       0      0     0           0      0    0  ...   \n",
      "3   0    0     0         0       0      0     0           0      0    0  ...   \n",
      "4   0    0     0         0       0      0     0           0      0    0  ...   \n",
      "\n",
      "   تنجح  حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  \n",
      "0     0    0     0   0   0   0        0   0    0        0  \n",
      "1     0    0     0   0   0   0        0   0    0        0  \n",
      "2     0    0     0   0   0   0        0   0    0        0  \n",
      "3     0    0     0   0   0   0        0   0    0        0  \n",
      "4     0    0     0   0   0   0        0   0    0        0  \n",
      "\n",
      "[5 rows x 55427 columns]\n",
      "    00       000  0000  00000031  000035  00006  0002  000billion  000ft  001  \\\n",
      "0  0.0  0.018552   0.0       0.0     0.0    0.0   0.0         0.0    0.0  0.0   \n",
      "1  0.0  0.000000   0.0       0.0     0.0    0.0   0.0         0.0    0.0  0.0   \n",
      "2  0.0  0.000000   0.0       0.0     0.0    0.0   0.0         0.0    0.0  0.0   \n",
      "3  0.0  0.000000   0.0       0.0     0.0    0.0   0.0         0.0    0.0  0.0   \n",
      "4  0.0  0.000000   0.0       0.0     0.0    0.0   0.0         0.0    0.0  0.0   \n",
      "\n",
      "   ...  تنجح  حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  \n",
      "0  ...   0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0  \n",
      "1  ...   0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0  \n",
      "2  ...   0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0  \n",
      "3  ...   0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0  \n",
      "4  ...   0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0  \n",
      "\n",
      "[5 rows x 55427 columns]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing the \"fake news\" model with CountVectorizer\n",
    "Now it's time to train the \"fake news\" model using the features we identified and extracted. We'll train and test a Naive Bayes model using the CountVectorizer data.\n",
    "\n",
    "The training and test sets have been created, and count_vectorizer, count_train, and count_test have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9025911708253359\n",
      "[[896 131]\n",
      " [ 72 985]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing the \"fake news\" model with TfidfVectorizer\n",
    "Now that we have evaluated the model using the CountVectorizer, we'll do the same using the TfidfVectorizer with a Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8512476007677543\n",
      "[[ 738  289]\n",
      " [  21 1036]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score and confusion matrix of Multinomial Naive Bayes classifier predictions trained on \n",
    "# tfidf_train, y_train and tested against tfidf_test and y_test\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', \"REAL\"])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the model\n",
    "We're going to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.05\n",
      "Score:  0.9064299424184261\n",
      "\n",
      "Alpha:  0.15000000000000002\n",
      "Score:  0.9069097888675623\n",
      "\n",
      "Alpha:  0.25000000000000006\n",
      "Score:  0.9064299424184261\n",
      "\n",
      "Alpha:  0.35000000000000003\n",
      "Score:  0.9073896353166987\n",
      "\n",
      "Alpha:  0.45000000000000007\n",
      "Score:  0.9069097888675623\n",
      "\n",
      "Alpha:  0.5500000000000002\n",
      "Score:  0.9054702495201535\n",
      "\n",
      "Alpha:  0.6500000000000001\n",
      "Score:  0.904510556621881\n",
      "\n",
      "Alpha:  0.7500000000000002\n",
      "Score:  0.9049904030710173\n",
      "\n",
      "Alpha:  0.8500000000000002\n",
      "Score:  0.9049904030710173\n",
      "\n",
      "Alpha:  0.9500000000000002\n",
      "Score:  0.9030710172744721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0.05, 1, 0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(count_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(count_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the model\n",
    "Now that we have built a \"fake news\" classifier, we'll investigate what it has learned. We can map the important vector weights back to actual words using some simple inspection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.300427527153001, '0000'), (-11.300427527153001, '000035'), (-11.300427527153001, '0002'), (-11.300427527153001, '000billion'), (-11.300427527153001, '0011'), (-11.300427527153001, '004s'), (-11.300427527153001, '005'), (-11.300427527153001, '005s'), (-11.300427527153001, '00684'), (-11.300427527153001, '006s'), (-11.300427527153001, '007s'), (-11.300427527153001, '008s'), (-11.300427527153001, '0099'), (-11.300427527153001, '00am'), (-11.300427527153001, '00p'), (-11.300427527153001, '00pm'), (-11.300427527153001, '013c2812c9'), (-11.300427527153001, '014'), (-11.300427527153001, '015'), (-11.300427527153001, '01am')]\n",
      "\n",
      "REAL [(-7.736122101029444, 'gop'), (-7.715082494631467, 'democratic'), (-7.700132178755215, 'states'), (-7.616062189000394, 'republicans'), (-7.613804573183871, 'voters'), (-7.602069929422667, 'house'), (-7.568255176935045, 'percent'), (-7.517735585484349, 'people'), (-7.504294941910381, 'new'), (-7.418655764303377, 'party'), (-7.416781588287513, 'cruz'), (-7.397424277583882, 'state'), (-7.322290546281074, 'republican'), (-7.301761494218299, 'campaign'), (-7.3006611179629175, 'president'), (-7.23024495158357, 'sanders'), (-7.095488841224384, 'obama'), (-6.725583327156366, 'clinton'), (-6.568115702747693, 'said'), (-6.320008556136388, 'trump')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "print()\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
